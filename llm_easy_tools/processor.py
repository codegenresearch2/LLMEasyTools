import json\nimport inspect\nimport traceback\nfrom concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\nfrom typing import Callable, Union, Optional, Any, get_origin, get_args\nfrom pprint import pprint\nfrom pydantic import BaseModel, Field, ValidationError\nfrom dataclasses import dataclass, field\n\nfrom llm_easy_tools.schema_generator import get_name, parameters_basemodel_from_function, LLMFunction\nfrom llm_easy_tools.types import ChatCompletion, ChatCompletionMessageToolCall, ChatCompletionMessage, ChatCompletionMessageToolCall, Function\n\nclass NoMatchingTool(Exception):\n    def __init__(self, message):\n        self.message = message\n        super().__init__(self.message)\n\n@dataclass\nclass ToolResult:\n    """\n    Represents the result of a tool invocation within the ToolBox framework.\n    \n    Attributes:\n        tool_call_id (str): A unique identifier for the tool call.\n        name (str): The name of the tool that was called.\n        output (Optional[Union[str, BaseModel]]): The output generated by the tool call, if any.\n        error (Optional[Exception]): An error message if the tool call failed.\n        stack_trace (Optional[str]): The stack trace if the tool call failed.\n        soft_errors (list[Exception]): A list of non-critical error messages encountered during the tool call.\n        prefix (Optional[BaseModel]): The Pydantic model instance used as a prefix in the tool call, if applicable.\n        tool (Optional[Union[Callable, BaseModel]]): The function or model that was called.\n    \n    Methods:\n        to_message(): Converts the ToolResult into a dictionary suitable for returning to a chat interface.\n    """\n    tool_call_id: str\n    name: str\n    output: Optional[Any] = None\n    arguments: Optional[dict[str, Any]] = None\n    error: Optional[Exception] = None\n    stack_trace: Optional[str] = None\n    soft_errors: list[Exception] = field(default_factory=list)\n    prefix: Optional[BaseModel] = None\n    tool: Optional[Union[Callable, BaseModel]] = None\n\n    def to_message(self) -> dict[str, str]:\n        if self.error is not None:\n            content = f"{self.error}"\n        elif self.output is None:\n            content = ""\n        elif isinstance(self.output, BaseModel):\n            content = f"{self.name} created"\n        else:\n            content = str(self.output)\n        return {"role": "tool", "tool_call_id": self.tool_call_id, "name": self.name, "content": content}\n\ndef process_tool_call(tool_call, functions_or_models, prefix_class=None, fix_json_args=True, case_insensitive=False) -> ToolResult:\n    function_call = tool_call.function\n    tool_name = function_call.name\n    args = function_call.arguments\n    soft_errors: list[Exception] = []\n    error = None\n    stack_trace = None\n    prefix = None\n    output = None\n    try:\n        tool_args = json.loads(args)\n    except json.decoder.JSONDecodeError as e:\n        if fix_json_args:\n            soft_errors.append(e)\n            args = args.replace(', }', '}').replace(',}', '}')\n            tool_args = json.loads(args)\n        else:\n            stack_trace = traceback.format_exc()\n            return ToolResult(tool_call_id=tool_call.id, name=tool_name, error=e, stack_trace=stack_trace)\n\n    if prefix_class is not None:\n        try:\n            prefix = _extract_prefix_unpacked(tool_args, prefix_class)\n        except ValidationError as e:\n            soft_errors.append(e)\n        prefix_name = prefix_class.__name__\n        if case_insensitive:\n            prefix_name = prefix_name.lower()\n        if not tool_name.startswith(prefix_name):\n            soft_errors.append(NoMatchingTool(f"Trying to decode function call with a name '{tool_name}' not matching prefix '{prefix_name}'"))\n        else:\n            tool_name = tool_name[len(prefix_name + '_and_'):]\n\n    tool = None\n\n    for f in functions_or_models:\n        if get_name(f, case_insensitive=case_insensitive) == tool_name:\n            tool = f\n            try:\n                output, new_soft_errors = _process_unpacked(f, tool_args, fix_json_args=fix_json_args)\n                soft_errors.extend(new_soft_errors)\n            except Exception as e:\n                error = e\n                stack_trace = traceback.format_exc()\n            break\n    else:\n        error = NoMatchingTool(f"Function {tool_name} not found")\n    result = ToolResult("""\n        tool_call_id=tool_call.id, \n        name=tool_name,\n        arguments=tool_args,\n        output=output, \n        error=error,\n        stack_trace=stack_trace,\n        soft_errors=soft_errors,\n        prefix=prefix,\n        tool=tool\n    """\n    return result\n\n# Helper functions\n\ndef _process_unpacked(function, tool_args={}, fix_json_args=True):\n    if isinstance(function, LLMFunction):\n        function = function.func\n    model = parameters_basemodel_from_function(function)\n    soft_errors = []\n    if fix_json_args:\n        for field, field_info in model.model_fields.items():\n            field_annotation = field_info.annotation\n            if _is_list_type(field_annotation):\n                if field in tool_args and isinstance(tool_args[field], str):\n                    tool_args[field] = split_string_to_list(tool_args[field])\n                    soft_errors.append(f"Fixed JSON decode error for field {field}")\n\n    model_instance = model(**tool_args)\n    args = {} \n    for field, _ in model.model_fields.items():\n        args[field] = getattr(model_instance, field)\n    return function(**args), soft_errors\n\n\ndef _is_list_type(annotation):\n    origin = get_origin(annotation)\n    args = get_args(annotation)\n\n    if origin is list:\n        return True\n    elif origin is Union or origin is Optional:\n        return any(_is_list_type(arg) for arg in args)\n    return False\n\n\ndef _extract_prefix_unpacked(tool_args, prefix_class):\n    prefix_args = {} \n    for key in list(tool_args.keys()):  # copy keys to list because we modify the dict while iterating over it\n        if key in prefix_class.__annotations__:\n            prefix_args[key] = tool_args.pop(key)\n    prefix = prefix_class(**prefix_args)\n    return prefix\n\n# Main block\nif __name__ == "__main__":\n    from llm_easy_tools.types import mk_chat_with_tool_call\n\n    def original_function():\n        return 'Result of function_decorated'\n\n    function_decorated = LLMFunction(original_function, name="altered_name")\n\n    class ExampleClass:\n        def simple_method(self, count: int, size: float):\n            """simple method does something"""\n            return 'Result of simple_method'\n\n    example_object = ExampleClass()\n\n    class User(BaseModel):\n        name: str\n        email: str\n\n\n    pprint(process_response(mk_chat_with_tool_call('altered_name', {}), [function_decorated]))\n    call_to_altered_name = mk_chat_with_tool_call('altered_name', {}).choices[0].message.tool_calls[0]\n    pprint(call_to_altered_name)\n    pprint(process_tool_call(call_to_altered_name, [function_decorated]))\n\n    call_to_simple_method = mk_chat_with_tool_call('simple_method', {"count": 1, "size": 2.2}).choices[0].message.tool_calls[0]\n    pprint(process_tool_call(call_to_simple_method, [example_object.simple_method]))\n\n    call_to_model = mk_chat_with_tool_call('User', {"name": 'John', "email": 'john@example.com'}).choices[0].message.tool_calls[0]\n    pprint(process_tool_call(call_to_model, [User]))